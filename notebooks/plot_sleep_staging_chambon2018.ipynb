{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Sleep staging on the Sleep Physionet dataset using Chambon2018 network\n",
        "\n",
        "This tutorial shows how to train and test a sleep staging neural network with\n",
        "Braindecode. We adapt the time distributed approach of [1]_ to learn on\n",
        "sequences of EEG windows using the openly accessible Sleep Physionet dataset\n",
        "[2]_ [3]_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Hubert Banville <hubert.jbanville@gmail.com>\n",
        "#\n",
        "# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n",
        "\n",
        "### Loading\n",
        "\n",
        "First, we load the data using the\n",
        ":class:`braindecode.datasets.sleep_physionet.SleepPhysionet` class. We load\n",
        "two recordings from two different individuals: we will use the first one to\n",
        "train our network and the second one to evaluate performance (as in the `MNE`_\n",
        "sleep staging example).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from numbers import Integral\n",
        "from braindecode.datasets import SleepPhysionet\n",
        "\n",
        "subject_ids = [0, 1]\n",
        "dataset = SleepPhysionet(\n",
        "    subject_ids=subject_ids, recording_ids=[2], crop_wake_mins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n",
        "\n",
        "Next, we preprocess the raw data. We convert the data to microvolts and apply\n",
        "a lowpass filter. We omit the downsampling step of [1]_ as the Sleep\n",
        "Physionet data is already sampled at a lower 100 Hz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import preprocess, Preprocessor\n",
        "from numpy import multiply\n",
        "\n",
        "high_cut_hz = 30\n",
        "factor = 1e6\n",
        "\n",
        "preprocessors = [\n",
        "    Preprocessor(lambda data: multiply(data, factor), apply_on_array=True),  # Convert from V to uV\n",
        "    Preprocessor('filter', l_freq=None, h_freq=high_cut_hz)\n",
        "]\n",
        "\n",
        "# Transform the data\n",
        "preprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract windows\n",
        "\n",
        "We extract 30-s windows to be used in the classification task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n",
        "\n",
        "mapping = {  # We merge stages 3 and 4 following AASM standards.\n",
        "    'Sleep stage W': 0,\n",
        "    'Sleep stage 1': 1,\n",
        "    'Sleep stage 2': 2,\n",
        "    'Sleep stage 3': 3,\n",
        "    'Sleep stage 4': 3,\n",
        "    'Sleep stage R': 4\n",
        "}\n",
        "\n",
        "window_size_s = 30\n",
        "sfreq = 100\n",
        "window_size_samples = window_size_s * sfreq\n",
        "\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset,\n",
        "    trial_start_offset_samples=0,\n",
        "    trial_stop_offset_samples=0,\n",
        "    window_size_samples=window_size_samples,\n",
        "    window_stride_samples=window_size_samples,\n",
        "    preload=True,\n",
        "    mapping=mapping\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window preprocessing\n",
        "\n",
        "We also preprocess the windows by applying channel-wise z-score normalization\n",
        "in each window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale as standard_scale\n",
        "\n",
        "preprocess(windows_dataset, [Preprocessor(standard_scale, channel_wise=True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and valid\n",
        "\n",
        "We split the dataset into training and validation set taking\n",
        "every other subject as train or valid.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split_ids = dict(train=subject_ids[::2], valid=subject_ids[1::2])\n",
        "splits = windows_dataset.split(split_ids)\n",
        "train_set, valid_set = splits[\"train\"], splits[\"valid\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create sequence samplers\n",
        "\n",
        "Following the time distributed approach of [1]_, we need to provide our\n",
        "neural network with sequences of windows, such that the embeddings of\n",
        "multiple consecutive windows can be concatenated and provided to a final\n",
        "classifier. We can achieve this by defining Sampler objects that return\n",
        "sequences of window indices.\n",
        "To simplify the example, we train the whole model end-to-end on sequences,\n",
        "rather than using the two-step approach of [1]_ (i.e. training the feature\n",
        "extractor on single windows, then freezing its weights and training the\n",
        "classifier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from braindecode.samplers import SequenceSampler\n",
        "\n",
        "n_windows = 3  # Sequences of 3 consecutive windows\n",
        "n_windows_stride = 3  # Maximally overlapping sequences\n",
        "\n",
        "train_sampler = SequenceSampler(\n",
        "    train_set.get_metadata(), n_windows, n_windows_stride, randomize=True\n",
        ")\n",
        "valid_sampler = SequenceSampler(valid_set.get_metadata(), n_windows, n_windows_stride)\n",
        "\n",
        "# Print number of examples per class\n",
        "print('Training examples: ', len(train_sampler))\n",
        "print('Validation examples: ', len(valid_sampler))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also implement a transform to extract the label of the center window of a\n",
        "sequence to use it as target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use label of center window in the sequence\n",
        "def get_center_label(x):\n",
        "    if isinstance(x, Integral):\n",
        "        return x\n",
        "    return x[np.ceil(len(x) / 2).astype(int)] if len(x) > 1 else x\n",
        "\n",
        "\n",
        "train_set.target_transform = get_center_label\n",
        "valid_set.target_transform = get_center_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, since some sleep stages appear a lot more often than others (e.g.\n",
        "most of the night is spent in the N2 stage), the classes are imbalanced. To\n",
        "avoid overfitting on the more frequent classes, we compute weights that we\n",
        "will provide to the loss function when training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "y_train = [train_set[idx][1] for idx in train_sampler]\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n",
        "\n",
        "We can now create the deep learning model. In this tutorial, we use the sleep\n",
        "staging architecture introduced in [1]_, which is a four-layer convolutional\n",
        "neural network. We use the time distributed version of the model, where the\n",
        "feature vectors of a sequence of windows are concatenated and passed to a\n",
        "linear layer for classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from braindecode.util import set_random_seeds\n",
        "from braindecode.models import SleepStagerChambon2018, TimeDistributed\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "# Set random seed to be able to roughly reproduce results\n",
        "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
        "# may still make results substantially different between runs.\n",
        "# To obtain more consistent results at the cost of increased computation time,\n",
        "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
        "# or remove `torch.backends.cudnn.benchmark = True`\n",
        "set_random_seeds(seed=31, cuda=cuda)\n",
        "\n",
        "n_classes = 5\n",
        "# Extract number of channels and time steps from dataset\n",
        "n_channels, input_size_samples = train_set[0][0].shape\n",
        "\n",
        "feat_extractor = SleepStagerChambon2018(\n",
        "    n_channels,\n",
        "    sfreq,\n",
        "    n_outputs=n_classes,\n",
        "    n_times=input_size_samples,\n",
        "    return_feats=True\n",
        ")\n",
        "\n",
        "model = nn.Sequential(\n",
        "    TimeDistributed(feat_extractor),  # apply model on each 30-s window\n",
        "    nn.Sequential(  # apply linear layer on concatenated feature vectors\n",
        "        nn.Flatten(start_dim=1),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(feat_extractor.len_last_layer * n_windows, n_classes)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "We can now train our network. :class:`braindecode.EEGClassifier` is a\n",
        "braindecode object that is responsible for managing the training of neural\n",
        "networks. It inherits from :class:`skorch.NeuralNetClassifier`, so the\n",
        "training logic is the same as in\n",
        "[Skorch](https://skorch.readthedocs.io/en/stable/)_.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>We use different hyperparameters from [1]_, as these hyperparameters were\n",
        "   optimized on a different dataset (MASS SS3) and with a different number of\n",
        "   recordings. Generally speaking, it is recommended to perform\n",
        "   hyperparameter optimization if reusing this code on a different dataset or\n",
        "   with more recordings.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.helper import predefined_split\n",
        "from skorch.callbacks import EpochScoring\n",
        "from braindecode import EEGClassifier\n",
        "\n",
        "lr = 1e-3\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "\n",
        "train_bal_acc = EpochScoring(\n",
        "    scoring='balanced_accuracy', on_train=True, name='train_bal_acc',\n",
        "    lower_is_better=False)\n",
        "valid_bal_acc = EpochScoring(\n",
        "    scoring='balanced_accuracy', on_train=False, name='valid_bal_acc',\n",
        "    lower_is_better=False)\n",
        "callbacks = [\n",
        "    ('train_bal_acc', train_bal_acc),\n",
        "    ('valid_bal_acc', valid_bal_acc)\n",
        "]\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.CrossEntropyLoss,\n",
        "    criterion__weight=torch.Tensor(class_weights).to(device),\n",
        "    optimizer=torch.optim.Adam,\n",
        "    iterator_train__shuffle=False,\n",
        "    iterator_train__sampler=train_sampler,\n",
        "    iterator_valid__sampler=valid_sampler,\n",
        "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
        "    optimizer__lr=lr,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks,\n",
        "    device=device,\n",
        "    classes=np.unique(y_train),\n",
        ")\n",
        "# Model training for a specified number of epochs. `y` is None as it is already\n",
        "# supplied in the dataset.\n",
        "clf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\n",
        "\n",
        "We use the history stored by Skorch during training to plot the performance of\n",
        "the model throughout training. Specifically, we plot the loss and the balanced\n",
        "balanced accuracy for the training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Extract loss and balanced accuracy values for plotting from history object\n",
        "df = pd.DataFrame(clf.history.to_list())\n",
        "df.index.name = \"Epoch\"\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 7), sharex=True)\n",
        "df[['train_loss', 'valid_loss']].plot(color=['r', 'b'], ax=ax1)\n",
        "df[['train_bal_acc', 'valid_bal_acc']].plot(color=['r', 'b'], ax=ax2)\n",
        "ax1.set_ylabel('Loss')\n",
        "ax2.set_ylabel('Balanced accuracy')\n",
        "ax1.legend(['Train', 'Valid'])\n",
        "ax2.legend(['Train', 'Valid'])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we also display the confusion matrix and classification report:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from braindecode.visualization import plot_confusion_matrix\n",
        "\n",
        "y_true = [valid_set[[i]][1][0] for i in range(len(valid_sampler))]\n",
        "y_pred = clf.predict(valid_set)\n",
        "\n",
        "confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plot_confusion_matrix(confusion_mat=confusion_mat,\n",
        "                      class_names=['Wake', 'N1', 'N2', 'N3', 'REM'])\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can also visualize the hypnogram of the recording we used for\n",
        "validation, with the predicted sleep stages overlaid on top of the true\n",
        "sleep stages. We can see that the model cannot correctly identify the\n",
        "different sleep stages with this amount of training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(y_true, color='b', label='Expert annotations')\n",
        "ax.plot(y_pred.flatten(), color='r', label='Predict annotations', alpha=0.5)\n",
        "ax.set_xlabel('Time (epochs)')\n",
        "ax.set_ylabel('Sleep stage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model was able to learn despite the low amount of data that was available\n",
        "(only two recordings in this example) and reached a balanced accuracy of\n",
        "about 36% in a 5-class classification task (chance-level = 20%) on held-out\n",
        "data.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To further improve performance, more recordings should be included in the\n",
        "   training set, and hyperparameters should be selected accordingly.\n",
        "   Increasing the sequence length was also shown in [1]_ to help improve\n",
        "   performance, especially when few EEG channels are available.</p></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        ".. [1] Chambon, S., Galtier, M., Arnal, P., Wainrib, G. and Gramfort, A.\n",
        "      (2018)A Deep Learning Architecture for Temporal Sleep Stage\n",
        "      Classification Using Multivariate and Multimodal Time Series.\n",
        "      IEEE Trans. on Neural Systems and Rehabilitation Engineering 26:\n",
        "      (758-769)\n",
        "\n",
        ".. [2] B Kemp, AH Zwinderman, B Tuk, HAC Kamphuisen, JJL Obery√©. Analysis of\n",
        "       a sleep-dependent neuronal feedback loop: the slow-wave\n",
        "       microcontinuity of the EEG. IEEE-BME 47(9):1185-1194 (2000).\n",
        "\n",
        ".. [3] Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh,\n",
        "       Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000)\n",
        "       PhysioBank, PhysioToolkit, and PhysioNet: Components of a New\n",
        "       Research Resource for Complex Physiologic Signals.\n",
        "       Circulation 101(23):e215-e220\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
